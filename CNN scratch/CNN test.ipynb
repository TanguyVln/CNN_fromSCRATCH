{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a237df4c-d3d6-474f-aa86-f34fa462b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 16:51:50.301145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737820310.319057   41379 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737820310.324344   41379 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-25 16:51:50.345559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e623fa85-94da-4ef4-b73a-a20186bc08e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87eab7e-c4c6-40ca-8d50-f505cb987dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_scratch2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fdbb20-aeb2-40ae-ac1a-a915f554bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = Conv_Layer(16)\n",
    "pool_layer = Max_Pool_Layer()\n",
    "softmax_layer = Softmax_Layer(2704, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3692b1-0bd2-4b77-8dab-99964a77b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, y):\n",
    "    out = conv_layer.forward((X/255) -0.5)\n",
    "    out = pool_layer.forward(out)\n",
    "    out = softmax_layer.forward(out)\n",
    "\n",
    "    loss = -np.log(out[y])\n",
    "    accuracy = 1 if(np.argmax(out) == y) else 0\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8554da-d82c-4c09-844d-c2ccc41564e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(grad, lr):\n",
    "    grad = softmax_layer.backward(grad, lr)\n",
    "    grad = pool_layer.backward(grad)\n",
    "    grad = conv_layer.backward(grad, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a335cfd-fbe8-4330-a56e-dbc8c05e7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, lr):\n",
    "    out, loss, accuracy = forward(X, y)\n",
    "\n",
    "    grad = np.zeros(10)\n",
    "    grad[y] = -1/out[y]\n",
    "\n",
    "    backward(grad, lr)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec6978a-c109-4042-a2a7-f52c25de513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "[Step 100] Past 100 steps: Average Loss 2.2332938110236533 | Accuracy: 23\n",
      "[Step 200] Past 100 steps: Average Loss 2.047214877067644 | Accuracy: 35\n",
      "[Step 300] Past 100 steps: Average Loss 1.5901809285590114 | Accuracy: 48\n",
      "[Step 400] Past 100 steps: Average Loss 1.1576528673888555 | Accuracy: 61\n",
      "[Step 500] Past 100 steps: Average Loss 1.006952662551246 | Accuracy: 65\n",
      "[Step 600] Past 100 steps: Average Loss 0.8503480877534995 | Accuracy: 76\n",
      "[Step 700] Past 100 steps: Average Loss 1.0431317064725913 | Accuracy: 65\n",
      "[Step 800] Past 100 steps: Average Loss 0.7006359289267103 | Accuracy: 79\n",
      "[Step 900] Past 100 steps: Average Loss 0.6298457733565135 | Accuracy: 82\n",
      "[Step 1000] Past 100 steps: Average Loss 0.6022605871773424 | Accuracy: 76\n",
      "[Step 1100] Past 100 steps: Average Loss 0.5737344292168691 | Accuracy: 84\n",
      "[Step 1200] Past 100 steps: Average Loss 0.430006836310601 | Accuracy: 86\n",
      "[Step 1300] Past 100 steps: Average Loss 0.6126959091233567 | Accuracy: 82\n",
      "[Step 1400] Past 100 steps: Average Loss 0.7258054879671033 | Accuracy: 78\n",
      "[Step 1500] Past 100 steps: Average Loss 0.5126009311699508 | Accuracy: 82\n",
      "[Step 1600] Past 100 steps: Average Loss 0.42381894272048726 | Accuracy: 85\n",
      "[Step 1700] Past 100 steps: Average Loss 0.4474097246497874 | Accuracy: 86\n",
      "[Step 1800] Past 100 steps: Average Loss 0.5625140489178855 | Accuracy: 80\n",
      "[Step 1900] Past 100 steps: Average Loss 0.6553472651758162 | Accuracy: 80\n",
      "[Step 2000] Past 100 steps: Average Loss 0.5506495465060808 | Accuracy: 84\n",
      "[Step 2100] Past 100 steps: Average Loss 0.5403983972148013 | Accuracy: 83\n",
      "[Step 2200] Past 100 steps: Average Loss 0.5075822029152495 | Accuracy: 83\n",
      "[Step 2300] Past 100 steps: Average Loss 0.44147854093254 | Accuracy: 89\n",
      "[Step 2400] Past 100 steps: Average Loss 0.5452135904384043 | Accuracy: 83\n",
      "[Step 2500] Past 100 steps: Average Loss 0.5330296117452873 | Accuracy: 83\n",
      "[Step 2600] Past 100 steps: Average Loss 0.6146515526622374 | Accuracy: 80\n",
      "[Step 2700] Past 100 steps: Average Loss 0.5224963710991287 | Accuracy: 87\n",
      "[Step 2800] Past 100 steps: Average Loss 0.46908665711791586 | Accuracy: 88\n",
      "[Step 2900] Past 100 steps: Average Loss 0.44650458879074734 | Accuracy: 88\n",
      "[Step 3000] Past 100 steps: Average Loss 0.3404223805021935 | Accuracy: 88\n",
      "[Step 3100] Past 100 steps: Average Loss 0.3732946430513588 | Accuracy: 88\n",
      "[Step 3200] Past 100 steps: Average Loss 0.5907417920253198 | Accuracy: 82\n",
      "[Step 3300] Past 100 steps: Average Loss 0.5071161009970698 | Accuracy: 85\n",
      "[Step 3400] Past 100 steps: Average Loss 0.4221231523964576 | Accuracy: 94\n",
      "[Step 3500] Past 100 steps: Average Loss 0.5701880760840161 | Accuracy: 86\n",
      "[Step 3600] Past 100 steps: Average Loss 0.45144061112431433 | Accuracy: 85\n",
      "[Step 3700] Past 100 steps: Average Loss 0.3459692092087869 | Accuracy: 87\n",
      "[Step 3800] Past 100 steps: Average Loss 0.419923381877018 | Accuracy: 83\n",
      "[Step 3900] Past 100 steps: Average Loss 0.33737260857869344 | Accuracy: 92\n",
      "[Step 4000] Past 100 steps: Average Loss 0.35721662508610164 | Accuracy: 87\n",
      "[Step 4100] Past 100 steps: Average Loss 0.5473939949240926 | Accuracy: 85\n",
      "[Step 4200] Past 100 steps: Average Loss 0.43987820873875705 | Accuracy: 85\n",
      "[Step 4300] Past 100 steps: Average Loss 0.4508936714573216 | Accuracy: 87\n",
      "[Step 4400] Past 100 steps: Average Loss 0.2947749558177696 | Accuracy: 92\n",
      "[Step 4500] Past 100 steps: Average Loss 0.5764225114336226 | Accuracy: 82\n",
      "[Step 4600] Past 100 steps: Average Loss 0.47067416935810624 | Accuracy: 84\n",
      "[Step 4700] Past 100 steps: Average Loss 0.4504309692786805 | Accuracy: 84\n",
      "[Step 4800] Past 100 steps: Average Loss 0.5377029974054841 | Accuracy: 83\n",
      "[Step 4900] Past 100 steps: Average Loss 0.5103480532353023 | Accuracy: 84\n",
      "[Step 5000] Past 100 steps: Average Loss 0.49656224946511324 | Accuracy: 87\n",
      "[Step 5100] Past 100 steps: Average Loss 0.30436055977429105 | Accuracy: 91\n",
      "[Step 5200] Past 100 steps: Average Loss 0.5305061816747618 | Accuracy: 85\n",
      "[Step 5300] Past 100 steps: Average Loss 0.3501903503864171 | Accuracy: 89\n",
      "[Step 5400] Past 100 steps: Average Loss 0.365547634919676 | Accuracy: 89\n",
      "[Step 5500] Past 100 steps: Average Loss 0.5066877309329442 | Accuracy: 83\n",
      "[Step 5600] Past 100 steps: Average Loss 0.4296272737038577 | Accuracy: 84\n",
      "[Step 5700] Past 100 steps: Average Loss 0.30913770685186226 | Accuracy: 93\n",
      "[Step 5800] Past 100 steps: Average Loss 0.3408749211341133 | Accuracy: 90\n",
      "[Step 5900] Past 100 steps: Average Loss 0.30172457457818974 | Accuracy: 88\n",
      "[Step 6000] Past 100 steps: Average Loss 0.21843182014308332 | Accuracy: 91\n",
      "[Step 6100] Past 100 steps: Average Loss 0.35162600530359517 | Accuracy: 89\n",
      "[Step 6200] Past 100 steps: Average Loss 0.23154981233828667 | Accuracy: 95\n",
      "[Step 6300] Past 100 steps: Average Loss 0.34851749703917734 | Accuracy: 86\n",
      "[Step 6400] Past 100 steps: Average Loss 0.40088582600717226 | Accuracy: 88\n",
      "[Step 6500] Past 100 steps: Average Loss 0.31138759351370254 | Accuracy: 87\n",
      "[Step 6600] Past 100 steps: Average Loss 0.26995302721094305 | Accuracy: 91\n",
      "[Step 6700] Past 100 steps: Average Loss 0.3444873973828071 | Accuracy: 91\n",
      "[Step 6800] Past 100 steps: Average Loss 0.3786026236248569 | Accuracy: 86\n",
      "[Step 6900] Past 100 steps: Average Loss 0.4049832016531966 | Accuracy: 87\n",
      "[Step 7000] Past 100 steps: Average Loss 0.3339597789903894 | Accuracy: 91\n",
      "[Step 7100] Past 100 steps: Average Loss 0.35872094606601296 | Accuracy: 90\n",
      "[Step 7200] Past 100 steps: Average Loss 0.2853659578794722 | Accuracy: 94\n",
      "[Step 7300] Past 100 steps: Average Loss 0.4783449118938753 | Accuracy: 87\n",
      "[Step 7400] Past 100 steps: Average Loss 0.2609586708996058 | Accuracy: 94\n",
      "[Step 7500] Past 100 steps: Average Loss 0.3075116648980684 | Accuracy: 90\n",
      "[Step 7600] Past 100 steps: Average Loss 0.36518158617412394 | Accuracy: 92\n",
      "[Step 7700] Past 100 steps: Average Loss 0.4101633507581599 | Accuracy: 89\n",
      "[Step 7800] Past 100 steps: Average Loss 0.4233320433428547 | Accuracy: 84\n",
      "[Step 7900] Past 100 steps: Average Loss 0.21129203225616525 | Accuracy: 93\n",
      "[Step 8000] Past 100 steps: Average Loss 0.3303402145941236 | Accuracy: 87\n",
      "[Step 8100] Past 100 steps: Average Loss 0.35970928758238396 | Accuracy: 89\n",
      "[Step 8200] Past 100 steps: Average Loss 0.2056137628412225 | Accuracy: 94\n",
      "[Step 8300] Past 100 steps: Average Loss 0.33502081735828115 | Accuracy: 92\n",
      "[Step 8400] Past 100 steps: Average Loss 0.2729328346158019 | Accuracy: 90\n",
      "[Step 8500] Past 100 steps: Average Loss 0.34185261216909085 | Accuracy: 90\n",
      "[Step 8600] Past 100 steps: Average Loss 0.37837993901634176 | Accuracy: 90\n",
      "[Step 8700] Past 100 steps: Average Loss 0.4121791245953553 | Accuracy: 87\n",
      "[Step 8800] Past 100 steps: Average Loss 0.2755080337288693 | Accuracy: 90\n",
      "[Step 8900] Past 100 steps: Average Loss 0.3319772358599835 | Accuracy: 92\n",
      "[Step 9000] Past 100 steps: Average Loss 0.33380442418716355 | Accuracy: 91\n",
      "[Step 9100] Past 100 steps: Average Loss 0.34670624864233945 | Accuracy: 87\n",
      "[Step 9200] Past 100 steps: Average Loss 0.27588854638926447 | Accuracy: 93\n",
      "[Step 9300] Past 100 steps: Average Loss 0.38115571553113065 | Accuracy: 91\n",
      "[Step 9400] Past 100 steps: Average Loss 0.2646251320829114 | Accuracy: 92\n",
      "[Step 9500] Past 100 steps: Average Loss 0.23893694434435367 | Accuracy: 94\n",
      "[Step 9600] Past 100 steps: Average Loss 0.23444563577138716 | Accuracy: 93\n",
      "[Step 9700] Past 100 steps: Average Loss 0.3949177075531322 | Accuracy: 89\n",
      "[Step 9800] Past 100 steps: Average Loss 0.3698138608041439 | Accuracy: 91\n",
      "[Step 9900] Past 100 steps: Average Loss 0.2138667065448447 | Accuracy: 94\n",
      "[Step 10000] Past 100 steps: Average Loss 0.22757920907848125 | Accuracy: 95\n",
      "[Step 10100] Past 100 steps: Average Loss 0.38753606473888225 | Accuracy: 92\n",
      "[Step 10200] Past 100 steps: Average Loss 0.35809384980866116 | Accuracy: 89\n",
      "[Step 10300] Past 100 steps: Average Loss 0.22390604311346546 | Accuracy: 91\n",
      "[Step 10400] Past 100 steps: Average Loss 0.48851785734944436 | Accuracy: 87\n",
      "[Step 10500] Past 100 steps: Average Loss 0.19049360781818225 | Accuracy: 96\n",
      "[Step 10600] Past 100 steps: Average Loss 0.22934990662375337 | Accuracy: 92\n",
      "[Step 10700] Past 100 steps: Average Loss 0.42019249226073574 | Accuracy: 88\n",
      "[Step 10800] Past 100 steps: Average Loss 0.345482342423516 | Accuracy: 86\n",
      "[Step 10900] Past 100 steps: Average Loss 0.39000853957792364 | Accuracy: 90\n",
      "[Step 11000] Past 100 steps: Average Loss 0.3435368123147815 | Accuracy: 89\n",
      "[Step 11100] Past 100 steps: Average Loss 0.2962309810138346 | Accuracy: 94\n",
      "[Step 11200] Past 100 steps: Average Loss 0.4223565430754552 | Accuracy: 84\n",
      "[Step 11300] Past 100 steps: Average Loss 0.4127961129456274 | Accuracy: 86\n",
      "[Step 11400] Past 100 steps: Average Loss 0.3691195887959883 | Accuracy: 88\n",
      "[Step 11500] Past 100 steps: Average Loss 0.22910655278355752 | Accuracy: 93\n",
      "[Step 11600] Past 100 steps: Average Loss 0.3748226851888448 | Accuracy: 89\n",
      "[Step 11700] Past 100 steps: Average Loss 0.2643489543090555 | Accuracy: 92\n",
      "[Step 11800] Past 100 steps: Average Loss 0.2763950608819164 | Accuracy: 93\n",
      "[Step 11900] Past 100 steps: Average Loss 0.3055933372223328 | Accuracy: 89\n",
      "[Step 12000] Past 100 steps: Average Loss 0.3655555332339027 | Accuracy: 86\n",
      "[Step 12100] Past 100 steps: Average Loss 0.22242755203614353 | Accuracy: 93\n",
      "[Step 12200] Past 100 steps: Average Loss 0.23980960586796363 | Accuracy: 93\n",
      "[Step 12300] Past 100 steps: Average Loss 0.32785735895552137 | Accuracy: 90\n",
      "[Step 12400] Past 100 steps: Average Loss 0.2690592447870243 | Accuracy: 89\n",
      "[Step 12500] Past 100 steps: Average Loss 0.1793646735589996 | Accuracy: 95\n",
      "[Step 12600] Past 100 steps: Average Loss 0.33260125183777317 | Accuracy: 90\n",
      "[Step 12700] Past 100 steps: Average Loss 0.2550066800233811 | Accuracy: 92\n",
      "[Step 12800] Past 100 steps: Average Loss 0.3307228954710919 | Accuracy: 88\n",
      "[Step 12900] Past 100 steps: Average Loss 0.3396108071870643 | Accuracy: 90\n",
      "[Step 13000] Past 100 steps: Average Loss 0.2862836744637439 | Accuracy: 92\n",
      "[Step 13100] Past 100 steps: Average Loss 0.4525618847155104 | Accuracy: 84\n",
      "[Step 13200] Past 100 steps: Average Loss 0.2648477729991636 | Accuracy: 92\n",
      "[Step 13300] Past 100 steps: Average Loss 0.49695379755521785 | Accuracy: 87\n",
      "[Step 13400] Past 100 steps: Average Loss 0.16685124079828914 | Accuracy: 94\n",
      "[Step 13500] Past 100 steps: Average Loss 0.31241204032279857 | Accuracy: 88\n",
      "[Step 13600] Past 100 steps: Average Loss 0.16446057608845482 | Accuracy: 96\n",
      "[Step 13700] Past 100 steps: Average Loss 0.23653128799412312 | Accuracy: 93\n",
      "[Step 13800] Past 100 steps: Average Loss 0.3832107537235543 | Accuracy: 85\n",
      "[Step 13900] Past 100 steps: Average Loss 0.4400462608294088 | Accuracy: 91\n",
      "[Step 14000] Past 100 steps: Average Loss 0.3504382767662453 | Accuracy: 91\n",
      "[Step 14100] Past 100 steps: Average Loss 0.2291414632220687 | Accuracy: 91\n",
      "[Step 14200] Past 100 steps: Average Loss 0.16387582956185223 | Accuracy: 96\n",
      "[Step 14300] Past 100 steps: Average Loss 0.17596474351783925 | Accuracy: 95\n",
      "[Step 14400] Past 100 steps: Average Loss 0.19865545279815866 | Accuracy: 92\n",
      "[Step 14500] Past 100 steps: Average Loss 0.3424304083344205 | Accuracy: 90\n",
      "[Step 14600] Past 100 steps: Average Loss 0.13032609313666466 | Accuracy: 96\n",
      "[Step 14700] Past 100 steps: Average Loss 0.2623360216173783 | Accuracy: 92\n",
      "[Step 14800] Past 100 steps: Average Loss 0.269906920659958 | Accuracy: 93\n",
      "[Step 14900] Past 100 steps: Average Loss 0.2569523109899012 | Accuracy: 92\n",
      "[Step 15000] Past 100 steps: Average Loss 0.3955176022874732 | Accuracy: 89\n",
      "[Step 15100] Past 100 steps: Average Loss 0.2190264242515157 | Accuracy: 89\n",
      "[Step 15200] Past 100 steps: Average Loss 0.1836133381041088 | Accuracy: 94\n",
      "[Step 15300] Past 100 steps: Average Loss 0.195446126572562 | Accuracy: 92\n",
      "[Step 15400] Past 100 steps: Average Loss 0.3115191353870892 | Accuracy: 90\n",
      "[Step 15500] Past 100 steps: Average Loss 0.38181435704118477 | Accuracy: 93\n",
      "[Step 15600] Past 100 steps: Average Loss 0.39936398183111876 | Accuracy: 84\n",
      "[Step 15700] Past 100 steps: Average Loss 0.42997846622979635 | Accuracy: 87\n",
      "[Step 15800] Past 100 steps: Average Loss 0.27416498065735434 | Accuracy: 90\n",
      "[Step 15900] Past 100 steps: Average Loss 0.27446553586587746 | Accuracy: 92\n",
      "[Step 16000] Past 100 steps: Average Loss 0.4244954039649776 | Accuracy: 85\n",
      "[Step 16100] Past 100 steps: Average Loss 0.17201230596847644 | Accuracy: 95\n",
      "[Step 16200] Past 100 steps: Average Loss 0.3744213397312465 | Accuracy: 90\n",
      "[Step 16300] Past 100 steps: Average Loss 0.3349467152988922 | Accuracy: 93\n",
      "[Step 16400] Past 100 steps: Average Loss 0.19960495842970227 | Accuracy: 94\n",
      "[Step 16500] Past 100 steps: Average Loss 0.1772889148614244 | Accuracy: 93\n",
      "[Step 16600] Past 100 steps: Average Loss 0.2363516846096156 | Accuracy: 90\n",
      "[Step 16700] Past 100 steps: Average Loss 0.149320212279634 | Accuracy: 96\n",
      "[Step 16800] Past 100 steps: Average Loss 0.3043135308384971 | Accuracy: 93\n",
      "[Step 16900] Past 100 steps: Average Loss 0.20360585169686196 | Accuracy: 94\n",
      "[Step 17000] Past 100 steps: Average Loss 0.226111693643187 | Accuracy: 94\n",
      "[Step 17100] Past 100 steps: Average Loss 0.23290359791956403 | Accuracy: 92\n",
      "[Step 17200] Past 100 steps: Average Loss 0.37067235242396884 | Accuracy: 88\n",
      "[Step 17300] Past 100 steps: Average Loss 0.24045767740800028 | Accuracy: 92\n",
      "[Step 17400] Past 100 steps: Average Loss 0.17981063503255357 | Accuracy: 95\n",
      "[Step 17500] Past 100 steps: Average Loss 0.20062778093807684 | Accuracy: 93\n",
      "[Step 17600] Past 100 steps: Average Loss 0.24154233686705562 | Accuracy: 93\n",
      "[Step 17700] Past 100 steps: Average Loss 0.2275846074997423 | Accuracy: 92\n",
      "[Step 17800] Past 100 steps: Average Loss 0.23211183258088947 | Accuracy: 93\n",
      "[Step 17900] Past 100 steps: Average Loss 0.37428621443805377 | Accuracy: 89\n",
      "[Step 18000] Past 100 steps: Average Loss 0.351239595072308 | Accuracy: 90\n",
      "[Step 18100] Past 100 steps: Average Loss 0.1520175576267509 | Accuracy: 95\n",
      "[Step 18200] Past 100 steps: Average Loss 0.3068892156571804 | Accuracy: 90\n",
      "[Step 18300] Past 100 steps: Average Loss 0.16453445779114248 | Accuracy: 95\n",
      "[Step 18400] Past 100 steps: Average Loss 0.22183902022694305 | Accuracy: 94\n",
      "[Step 18500] Past 100 steps: Average Loss 0.2032231922794937 | Accuracy: 93\n",
      "[Step 18600] Past 100 steps: Average Loss 0.24311969153639068 | Accuracy: 94\n",
      "[Step 18700] Past 100 steps: Average Loss 0.18084839165837482 | Accuracy: 95\n",
      "[Step 18800] Past 100 steps: Average Loss 0.16593754229685698 | Accuracy: 96\n",
      "[Step 18900] Past 100 steps: Average Loss 0.32775926714218534 | Accuracy: 94\n",
      "[Step 19000] Past 100 steps: Average Loss 0.22551372965374492 | Accuracy: 94\n",
      "[Step 19100] Past 100 steps: Average Loss 0.35258482997906415 | Accuracy: 89\n",
      "[Step 19200] Past 100 steps: Average Loss 0.23399275628199637 | Accuracy: 96\n",
      "[Step 19300] Past 100 steps: Average Loss 0.15845639819133692 | Accuracy: 97\n",
      "[Step 19400] Past 100 steps: Average Loss 0.11801854569452251 | Accuracy: 95\n",
      "[Step 19500] Past 100 steps: Average Loss 0.3484900644000172 | Accuracy: 88\n",
      "[Step 19600] Past 100 steps: Average Loss 0.24962829842381326 | Accuracy: 96\n",
      "[Step 19700] Past 100 steps: Average Loss 0.2932166694358801 | Accuracy: 87\n",
      "[Step 19800] Past 100 steps: Average Loss 0.24173919650510634 | Accuracy: 92\n",
      "[Step 19900] Past 100 steps: Average Loss 0.15215238598065634 | Accuracy: 96\n",
      "[Step 20000] Past 100 steps: Average Loss 0.14854426043146643 | Accuracy: 95\n",
      "[Step 20100] Past 100 steps: Average Loss 0.25499776872129487 | Accuracy: 92\n",
      "[Step 20200] Past 100 steps: Average Loss 0.3993925194750075 | Accuracy: 89\n",
      "[Step 20300] Past 100 steps: Average Loss 0.16836048673145482 | Accuracy: 94\n",
      "[Step 20400] Past 100 steps: Average Loss 0.168659185744601 | Accuracy: 92\n",
      "[Step 20500] Past 100 steps: Average Loss 0.21012993039467578 | Accuracy: 93\n",
      "[Step 20600] Past 100 steps: Average Loss 0.2457100298250846 | Accuracy: 93\n",
      "[Step 20700] Past 100 steps: Average Loss 0.17698250503164695 | Accuracy: 92\n",
      "[Step 20800] Past 100 steps: Average Loss 0.3233807314496486 | Accuracy: 91\n",
      "[Step 20900] Past 100 steps: Average Loss 0.17870310665150752 | Accuracy: 96\n",
      "[Step 21000] Past 100 steps: Average Loss 0.218439918511036 | Accuracy: 94\n",
      "[Step 21100] Past 100 steps: Average Loss 0.17150688696100874 | Accuracy: 93\n",
      "[Step 21200] Past 100 steps: Average Loss 0.25413224151065417 | Accuracy: 91\n",
      "[Step 21300] Past 100 steps: Average Loss 0.2538069038349227 | Accuracy: 92\n",
      "[Step 21400] Past 100 steps: Average Loss 0.25307398016831517 | Accuracy: 93\n",
      "[Step 21500] Past 100 steps: Average Loss 0.123138461521648 | Accuracy: 97\n",
      "[Step 21600] Past 100 steps: Average Loss 0.24563656834232916 | Accuracy: 94\n",
      "[Step 21700] Past 100 steps: Average Loss 0.17683638973555424 | Accuracy: 95\n",
      "[Step 21800] Past 100 steps: Average Loss 0.16070119896083995 | Accuracy: 94\n",
      "[Step 21900] Past 100 steps: Average Loss 0.26646477686416803 | Accuracy: 90\n",
      "[Step 22000] Past 100 steps: Average Loss 0.11740858881073062 | Accuracy: 98\n",
      "[Step 22100] Past 100 steps: Average Loss 0.22006844516529317 | Accuracy: 92\n",
      "[Step 22200] Past 100 steps: Average Loss 0.05139629201665726 | Accuracy: 100\n",
      "[Step 22300] Past 100 steps: Average Loss 0.3186735638040143 | Accuracy: 91\n",
      "[Step 22400] Past 100 steps: Average Loss 0.34965609843847517 | Accuracy: 90\n",
      "[Step 22500] Past 100 steps: Average Loss 0.17742016024334606 | Accuracy: 94\n",
      "[Step 22600] Past 100 steps: Average Loss 0.12733226253842544 | Accuracy: 97\n",
      "[Step 22700] Past 100 steps: Average Loss 0.3460914179971825 | Accuracy: 91\n",
      "[Step 22800] Past 100 steps: Average Loss 0.11722130809280333 | Accuracy: 95\n",
      "[Step 22900] Past 100 steps: Average Loss 0.21938380402227584 | Accuracy: 94\n",
      "[Step 23000] Past 100 steps: Average Loss 0.21052994419565518 | Accuracy: 95\n",
      "[Step 23100] Past 100 steps: Average Loss 0.24209050809532193 | Accuracy: 92\n",
      "[Step 23200] Past 100 steps: Average Loss 0.22135333030817508 | Accuracy: 96\n",
      "[Step 23300] Past 100 steps: Average Loss 0.17241775813162755 | Accuracy: 95\n",
      "[Step 23400] Past 100 steps: Average Loss 0.22105573483787133 | Accuracy: 92\n",
      "[Step 23500] Past 100 steps: Average Loss 0.2689093626165485 | Accuracy: 92\n",
      "[Step 23600] Past 100 steps: Average Loss 0.1765084815141913 | Accuracy: 95\n",
      "[Step 23700] Past 100 steps: Average Loss 0.15693116433390517 | Accuracy: 93\n",
      "[Step 23800] Past 100 steps: Average Loss 0.2364822726848994 | Accuracy: 94\n",
      "[Step 23900] Past 100 steps: Average Loss 0.2161729798997782 | Accuracy: 95\n",
      "[Step 24000] Past 100 steps: Average Loss 0.24192066757690556 | Accuracy: 94\n",
      "[Step 24100] Past 100 steps: Average Loss 0.2734514295839149 | Accuracy: 91\n",
      "[Step 24200] Past 100 steps: Average Loss 0.2620147956234715 | Accuracy: 94\n",
      "[Step 24300] Past 100 steps: Average Loss 0.15711513249606351 | Accuracy: 95\n",
      "[Step 24400] Past 100 steps: Average Loss 0.2698368970592787 | Accuracy: 91\n",
      "[Step 24500] Past 100 steps: Average Loss 0.31111061394961365 | Accuracy: 92\n",
      "[Step 24600] Past 100 steps: Average Loss 0.32986110419593806 | Accuracy: 92\n",
      "[Step 24700] Past 100 steps: Average Loss 0.275286309586632 | Accuracy: 91\n",
      "[Step 24800] Past 100 steps: Average Loss 0.2235888871154319 | Accuracy: 93\n",
      "[Step 24900] Past 100 steps: Average Loss 0.3208384430131275 | Accuracy: 89\n",
      "[Step 25000] Past 100 steps: Average Loss 0.24169794643056064 | Accuracy: 94\n",
      "[Step 25100] Past 100 steps: Average Loss 0.22604816850247242 | Accuracy: 91\n",
      "[Step 25200] Past 100 steps: Average Loss 0.12444076584607972 | Accuracy: 96\n",
      "[Step 25300] Past 100 steps: Average Loss 0.1931030501776235 | Accuracy: 96\n",
      "[Step 25400] Past 100 steps: Average Loss 0.12653663735121598 | Accuracy: 96\n",
      "[Step 25500] Past 100 steps: Average Loss 0.25928245005094963 | Accuracy: 92\n",
      "[Step 25600] Past 100 steps: Average Loss 0.3938893998114596 | Accuracy: 86\n",
      "[Step 25700] Past 100 steps: Average Loss 0.22704100574566755 | Accuracy: 93\n",
      "[Step 25800] Past 100 steps: Average Loss 0.13249763173627271 | Accuracy: 97\n",
      "[Step 25900] Past 100 steps: Average Loss 0.2954430337113488 | Accuracy: 90\n",
      "[Step 26000] Past 100 steps: Average Loss 0.2655161926256317 | Accuracy: 94\n",
      "[Step 26100] Past 100 steps: Average Loss 0.11714788674425948 | Accuracy: 98\n",
      "[Step 26200] Past 100 steps: Average Loss 0.14545358283040782 | Accuracy: 94\n",
      "[Step 26300] Past 100 steps: Average Loss 0.39748287657865666 | Accuracy: 89\n",
      "[Step 26400] Past 100 steps: Average Loss 0.12923575999069198 | Accuracy: 96\n",
      "[Step 26500] Past 100 steps: Average Loss 0.25233454099937597 | Accuracy: 94\n",
      "[Step 26600] Past 100 steps: Average Loss 0.26546810110727703 | Accuracy: 91\n",
      "[Step 26700] Past 100 steps: Average Loss 0.2464231289295404 | Accuracy: 93\n",
      "[Step 26800] Past 100 steps: Average Loss 0.3027912267346236 | Accuracy: 89\n",
      "[Step 26900] Past 100 steps: Average Loss 0.2031280511147952 | Accuracy: 95\n",
      "[Step 27000] Past 100 steps: Average Loss 0.2110196201934244 | Accuracy: 93\n",
      "[Step 27100] Past 100 steps: Average Loss 0.32387090463327506 | Accuracy: 91\n",
      "[Step 27200] Past 100 steps: Average Loss 0.2229190332534152 | Accuracy: 95\n",
      "[Step 27300] Past 100 steps: Average Loss 0.20580740927050772 | Accuracy: 94\n",
      "[Step 27400] Past 100 steps: Average Loss 0.19356951919467058 | Accuracy: 94\n",
      "[Step 27500] Past 100 steps: Average Loss 0.13069430112320682 | Accuracy: 95\n",
      "[Step 27600] Past 100 steps: Average Loss 0.21066391807418441 | Accuracy: 95\n",
      "[Step 27700] Past 100 steps: Average Loss 0.16651398061539308 | Accuracy: 95\n",
      "[Step 27800] Past 100 steps: Average Loss 0.10074538270042951 | Accuracy: 98\n",
      "[Step 27900] Past 100 steps: Average Loss 0.27922650896405676 | Accuracy: 91\n",
      "[Step 28000] Past 100 steps: Average Loss 0.26374560791631124 | Accuracy: 93\n",
      "[Step 28100] Past 100 steps: Average Loss 0.12690019121404528 | Accuracy: 95\n",
      "[Step 28200] Past 100 steps: Average Loss 0.2988195433632346 | Accuracy: 92\n",
      "[Step 28300] Past 100 steps: Average Loss 0.20263736573770116 | Accuracy: 90\n",
      "[Step 28400] Past 100 steps: Average Loss 0.09799977398330953 | Accuracy: 97\n",
      "[Step 28500] Past 100 steps: Average Loss 0.07489974585897549 | Accuracy: 99\n",
      "[Step 28600] Past 100 steps: Average Loss 0.07123081524119128 | Accuracy: 98\n",
      "[Step 28700] Past 100 steps: Average Loss 0.16908484260006987 | Accuracy: 97\n",
      "[Step 28800] Past 100 steps: Average Loss 0.1723735217058713 | Accuracy: 94\n",
      "[Step 28900] Past 100 steps: Average Loss 0.11662469559652384 | Accuracy: 97\n",
      "[Step 29000] Past 100 steps: Average Loss 0.12675886860707922 | Accuracy: 96\n",
      "[Step 29100] Past 100 steps: Average Loss 0.21449754238726154 | Accuracy: 94\n",
      "[Step 29200] Past 100 steps: Average Loss 0.20325614833707165 | Accuracy: 95\n",
      "[Step 29300] Past 100 steps: Average Loss 0.19495794700096333 | Accuracy: 94\n",
      "[Step 29400] Past 100 steps: Average Loss 0.24898059348633386 | Accuracy: 92\n",
      "[Step 29500] Past 100 steps: Average Loss 0.16168544626586503 | Accuracy: 92\n",
      "[Step 29600] Past 100 steps: Average Loss 0.19569577915449177 | Accuracy: 94\n",
      "[Step 29700] Past 100 steps: Average Loss 0.19578974315997436 | Accuracy: 95\n",
      "[Step 29800] Past 100 steps: Average Loss 0.21077953438493005 | Accuracy: 92\n",
      "[Step 29900] Past 100 steps: Average Loss 0.17301289377503642 | Accuracy: 95\n",
      "[Step 30000] Past 100 steps: Average Loss 0.24909070632239882 | Accuracy: 94\n",
      "[Step 30100] Past 100 steps: Average Loss 0.0883353246462852 | Accuracy: 97\n",
      "[Step 30200] Past 100 steps: Average Loss 0.20099488461335113 | Accuracy: 95\n",
      "[Step 30300] Past 100 steps: Average Loss 0.19238165001272367 | Accuracy: 94\n",
      "[Step 30400] Past 100 steps: Average Loss 0.2019964639906291 | Accuracy: 95\n",
      "[Step 30500] Past 100 steps: Average Loss 0.2185673317460354 | Accuracy: 93\n",
      "[Step 30600] Past 100 steps: Average Loss 0.25263152706708036 | Accuracy: 91\n",
      "[Step 30700] Past 100 steps: Average Loss 0.2725811922476093 | Accuracy: 93\n",
      "[Step 30800] Past 100 steps: Average Loss 0.22938603128154242 | Accuracy: 95\n",
      "[Step 30900] Past 100 steps: Average Loss 0.1866092707054765 | Accuracy: 94\n",
      "[Step 31000] Past 100 steps: Average Loss 0.18118122650048726 | Accuracy: 94\n",
      "[Step 31100] Past 100 steps: Average Loss 0.2269136010605277 | Accuracy: 95\n",
      "[Step 31200] Past 100 steps: Average Loss 0.2646954894300604 | Accuracy: 91\n",
      "[Step 31300] Past 100 steps: Average Loss 0.1948483934927312 | Accuracy: 94\n",
      "[Step 31400] Past 100 steps: Average Loss 0.23760793397118532 | Accuracy: 94\n",
      "[Step 31500] Past 100 steps: Average Loss 0.18139229727235867 | Accuracy: 97\n",
      "[Step 31600] Past 100 steps: Average Loss 0.12641897824742013 | Accuracy: 97\n",
      "[Step 31700] Past 100 steps: Average Loss 0.221217641953053 | Accuracy: 93\n",
      "[Step 31800] Past 100 steps: Average Loss 0.11785760459901302 | Accuracy: 95\n",
      "[Step 31900] Past 100 steps: Average Loss 0.22177066855874325 | Accuracy: 96\n",
      "[Step 32000] Past 100 steps: Average Loss 0.13331078345233047 | Accuracy: 96\n",
      "[Step 32100] Past 100 steps: Average Loss 0.2007922826841731 | Accuracy: 93\n",
      "[Step 32200] Past 100 steps: Average Loss 0.15638958990996754 | Accuracy: 96\n",
      "[Step 32300] Past 100 steps: Average Loss 0.2469148816348429 | Accuracy: 93\n",
      "[Step 32400] Past 100 steps: Average Loss 0.2585031763861959 | Accuracy: 90\n",
      "[Step 32500] Past 100 steps: Average Loss 0.12838478121198435 | Accuracy: 97\n",
      "[Step 32600] Past 100 steps: Average Loss 0.14566769411715427 | Accuracy: 97\n",
      "[Step 32700] Past 100 steps: Average Loss 0.16847425239084857 | Accuracy: 95\n",
      "[Step 32800] Past 100 steps: Average Loss 0.15951026810623764 | Accuracy: 95\n",
      "[Step 32900] Past 100 steps: Average Loss 0.39014094177032554 | Accuracy: 93\n",
      "[Step 33000] Past 100 steps: Average Loss 0.2141641250432842 | Accuracy: 95\n",
      "[Step 33100] Past 100 steps: Average Loss 0.225796697486453 | Accuracy: 96\n",
      "[Step 33200] Past 100 steps: Average Loss 0.09848058463522316 | Accuracy: 97\n",
      "[Step 33300] Past 100 steps: Average Loss 0.3531896957946259 | Accuracy: 90\n",
      "[Step 33400] Past 100 steps: Average Loss 0.11832141522971952 | Accuracy: 96\n",
      "[Step 33500] Past 100 steps: Average Loss 0.3013715707102678 | Accuracy: 93\n",
      "[Step 33600] Past 100 steps: Average Loss 0.17662993987865805 | Accuracy: 97\n",
      "[Step 33700] Past 100 steps: Average Loss 0.1691134055963367 | Accuracy: 94\n",
      "[Step 33800] Past 100 steps: Average Loss 0.2470516716339966 | Accuracy: 92\n",
      "[Step 33900] Past 100 steps: Average Loss 0.18656296909558118 | Accuracy: 94\n",
      "[Step 34000] Past 100 steps: Average Loss 0.20109618676962884 | Accuracy: 94\n",
      "[Step 34100] Past 100 steps: Average Loss 0.12233799996416299 | Accuracy: 96\n",
      "[Step 34200] Past 100 steps: Average Loss 0.28606326317594244 | Accuracy: 96\n",
      "[Step 34300] Past 100 steps: Average Loss 0.21334917797285585 | Accuracy: 93\n",
      "[Step 34400] Past 100 steps: Average Loss 0.18373275145642018 | Accuracy: 94\n",
      "[Step 34500] Past 100 steps: Average Loss 0.19281618209387008 | Accuracy: 97\n",
      "[Step 34600] Past 100 steps: Average Loss 0.0660727837748996 | Accuracy: 99\n",
      "[Step 34700] Past 100 steps: Average Loss 0.3340571397876934 | Accuracy: 90\n",
      "[Step 34800] Past 100 steps: Average Loss 0.2682474931555064 | Accuracy: 94\n",
      "[Step 34900] Past 100 steps: Average Loss 0.2706383012168332 | Accuracy: 93\n",
      "[Step 35000] Past 100 steps: Average Loss 0.19019181362843052 | Accuracy: 91\n",
      "[Step 35100] Past 100 steps: Average Loss 0.12435674803780188 | Accuracy: 95\n",
      "[Step 35200] Past 100 steps: Average Loss 0.12606395823045174 | Accuracy: 96\n",
      "[Step 35300] Past 100 steps: Average Loss 0.22290999754261548 | Accuracy: 94\n",
      "[Step 35400] Past 100 steps: Average Loss 0.16090785302551291 | Accuracy: 96\n",
      "[Step 35500] Past 100 steps: Average Loss 0.24497526503679748 | Accuracy: 94\n",
      "[Step 35600] Past 100 steps: Average Loss 0.1114108509753751 | Accuracy: 96\n",
      "[Step 35700] Past 100 steps: Average Loss 0.13886940565829997 | Accuracy: 95\n",
      "[Step 35800] Past 100 steps: Average Loss 0.2013294377086316 | Accuracy: 94\n",
      "[Step 35900] Past 100 steps: Average Loss 0.13919527133266818 | Accuracy: 95\n",
      "[Step 36000] Past 100 steps: Average Loss 0.0730983596696972 | Accuracy: 99\n",
      "[Step 36100] Past 100 steps: Average Loss 0.13747976314480603 | Accuracy: 94\n",
      "[Step 36200] Past 100 steps: Average Loss 0.14282182120118703 | Accuracy: 94\n",
      "[Step 36300] Past 100 steps: Average Loss 0.13976406661884747 | Accuracy: 98\n",
      "[Step 36400] Past 100 steps: Average Loss 0.25148448558368397 | Accuracy: 97\n",
      "[Step 36500] Past 100 steps: Average Loss 0.11535442123727262 | Accuracy: 97\n",
      "[Step 36600] Past 100 steps: Average Loss 0.137474508850532 | Accuracy: 95\n",
      "[Step 36700] Past 100 steps: Average Loss 0.07030727392554152 | Accuracy: 98\n",
      "[Step 36800] Past 100 steps: Average Loss 0.16979262628706673 | Accuracy: 92\n",
      "[Step 36900] Past 100 steps: Average Loss 0.1434693233773568 | Accuracy: 97\n",
      "[Step 37000] Past 100 steps: Average Loss 0.1278410921810968 | Accuracy: 96\n",
      "[Step 37100] Past 100 steps: Average Loss 0.1759418072416764 | Accuracy: 96\n",
      "[Step 37200] Past 100 steps: Average Loss 0.13699151527927772 | Accuracy: 94\n",
      "[Step 37300] Past 100 steps: Average Loss 0.08533470520183124 | Accuracy: 96\n",
      "[Step 37400] Past 100 steps: Average Loss 0.1334510380221785 | Accuracy: 95\n",
      "[Step 37500] Past 100 steps: Average Loss 0.23842382371273943 | Accuracy: 92\n",
      "[Step 37600] Past 100 steps: Average Loss 0.15799882504765186 | Accuracy: 94\n",
      "[Step 37700] Past 100 steps: Average Loss 0.13880838596655315 | Accuracy: 96\n",
      "[Step 37800] Past 100 steps: Average Loss 0.30742089139634243 | Accuracy: 94\n",
      "[Step 37900] Past 100 steps: Average Loss 0.12295504758629976 | Accuracy: 94\n",
      "[Step 38000] Past 100 steps: Average Loss 0.13776984561039962 | Accuracy: 94\n",
      "[Step 38100] Past 100 steps: Average Loss 0.14783739501867352 | Accuracy: 96\n",
      "[Step 38200] Past 100 steps: Average Loss 0.2539335429729512 | Accuracy: 92\n",
      "[Step 38300] Past 100 steps: Average Loss 0.2048651929442881 | Accuracy: 92\n",
      "[Step 38400] Past 100 steps: Average Loss 0.13395494997207355 | Accuracy: 97\n",
      "[Step 38500] Past 100 steps: Average Loss 0.18566475040705208 | Accuracy: 94\n",
      "[Step 38600] Past 100 steps: Average Loss 0.10862007902524773 | Accuracy: 97\n",
      "[Step 38700] Past 100 steps: Average Loss 0.07794747794338995 | Accuracy: 99\n",
      "[Step 38800] Past 100 steps: Average Loss 0.15180933353854104 | Accuracy: 95\n",
      "[Step 38900] Past 100 steps: Average Loss 0.12626635433782052 | Accuracy: 96\n",
      "[Step 39000] Past 100 steps: Average Loss 0.10931582853837465 | Accuracy: 96\n",
      "[Step 39100] Past 100 steps: Average Loss 0.30179431769439397 | Accuracy: 92\n",
      "[Step 39200] Past 100 steps: Average Loss 0.2031347521495161 | Accuracy: 93\n",
      "[Step 39300] Past 100 steps: Average Loss 0.13361367282275197 | Accuracy: 98\n",
      "[Step 39400] Past 100 steps: Average Loss 0.4117361487928049 | Accuracy: 86\n",
      "[Step 39500] Past 100 steps: Average Loss 0.12101861604241972 | Accuracy: 95\n",
      "[Step 39600] Past 100 steps: Average Loss 0.20192838825588263 | Accuracy: 91\n",
      "[Step 39700] Past 100 steps: Average Loss 0.1134044753799402 | Accuracy: 97\n",
      "[Step 39800] Past 100 steps: Average Loss 0.08034946344252064 | Accuracy: 97\n",
      "[Step 39900] Past 100 steps: Average Loss 0.1623252943134704 | Accuracy: 93\n",
      "[Step 40000] Past 100 steps: Average Loss 0.18802978020389743 | Accuracy: 95\n",
      "[Step 40100] Past 100 steps: Average Loss 0.15455051240490314 | Accuracy: 96\n",
      "[Step 40200] Past 100 steps: Average Loss 0.22847461988192552 | Accuracy: 93\n",
      "[Step 40300] Past 100 steps: Average Loss 0.09094402688731586 | Accuracy: 98\n",
      "[Step 40400] Past 100 steps: Average Loss 0.1768800374854972 | Accuracy: 97\n",
      "[Step 40500] Past 100 steps: Average Loss 0.07343193180318287 | Accuracy: 98\n",
      "[Step 40600] Past 100 steps: Average Loss 0.1738020051113465 | Accuracy: 94\n",
      "[Step 40700] Past 100 steps: Average Loss 0.1399095721283858 | Accuracy: 95\n",
      "[Step 40800] Past 100 steps: Average Loss 0.14275913698975415 | Accuracy: 94\n",
      "[Step 40900] Past 100 steps: Average Loss 0.23915783808251054 | Accuracy: 95\n",
      "[Step 41000] Past 100 steps: Average Loss 0.24875940728719823 | Accuracy: 92\n",
      "[Step 41100] Past 100 steps: Average Loss 0.14160050061787463 | Accuracy: 95\n",
      "[Step 41200] Past 100 steps: Average Loss 0.2504897983128889 | Accuracy: 95\n",
      "[Step 41300] Past 100 steps: Average Loss 0.1821150381752094 | Accuracy: 95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m l, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m     19\u001b[0m accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, y, lr)\u001b[0m\n\u001b[1;32m      4\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m grad[y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mout[y]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(grad, lr)\u001b[0m\n\u001b[1;32m      2\u001b[0m grad \u001b[38;5;241m=\u001b[39m softmax_layer\u001b[38;5;241m.\u001b[39mbackward(grad, lr)\n\u001b[1;32m      3\u001b[0m grad \u001b[38;5;241m=\u001b[39m pool_layer\u001b[38;5;241m.\u001b[39mbackward(grad)\n\u001b[0;32m----> 4\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/CNN scratch/CNN_scratch2.py:35\u001b[0m, in \u001b[0;36mConv_Layer.backward\u001b[0;34m(self, d_out, learn_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters):\n\u001b[0;32m---> 35\u001b[0m             d_filters[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_out[i, j, k] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, j:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learn_rate \u001b[38;5;241m*\u001b[39m d_filters\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'epoch{i+1}')\n",
    "    \n",
    "    permutation = np.random.permutation(len(train_X))\n",
    "    train_X = train_X[permutation]\n",
    "    train_y = train_y[permutation]\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for j, (X, y) in enumerate(zip(train_X, train_y)):\n",
    "        if(j>0 and j %100 == 99):\n",
    "            print(f'[Step {j+1}] Past 100 steps: Average Loss {loss/100} | Accuracy: {accuracy}')\n",
    "            loss = 0\n",
    "            accuracy = 0\n",
    "        l, acc = train(X, y, 0.005)\n",
    "        loss += l\n",
    "        accuracy += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3018277-131a-4ac3-9272-c3a8372ca2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
