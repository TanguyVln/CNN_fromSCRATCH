{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a237df4c-d3d6-474f-aa86-f34fa462b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 12:57:06.101459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738151826.141167    6459 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738151826.152751    6459 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 12:57:06.195024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e623fa85-94da-4ef4-b73a-a20186bc08e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87eab7e-c4c6-40ca-8d50-f505cb987dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_scratch2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fdbb20-aeb2-40ae-ac1a-a915f554bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = Conv_Layer(16)\n",
    "pool_layer = Max_Pool_Layer()\n",
    "fc_layer = FC_layer(2704, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3692b1-0bd2-4b77-8dab-99964a77b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, y):\n",
    "    out = conv_layer.forward((X/255) -0.5)\n",
    "    out = pool_layer.forward(out)\n",
    "    out = fc_layer.forward(out)\n",
    "\n",
    "    loss = -np.log(out[y])\n",
    "    accuracy = 1 if(np.argmax(out) == y) else 0\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8554da-d82c-4c09-844d-c2ccc41564e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(grad, lr):\n",
    "    grad = fc_layer.backward(grad, lr)\n",
    "    grad = pool_layer.backward(grad)\n",
    "    grad = conv_layer.backward(grad, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a335cfd-fbe8-4330-a56e-dbc8c05e7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, lr):\n",
    "    out, loss, accuracy = forward(X, y)\n",
    "\n",
    "    grad = np.zeros(10)\n",
    "    grad[y] = -1/out[y]\n",
    "\n",
    "    backward(grad, lr)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3018277-131a-4ac3-9272-c3a8372ca2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100] Past 100 steps: Average Loss 4.888390626006086 | Accuracy: 35\n",
      "[Step 200] Past 100 steps: Average Loss 1.000749893155044 | Accuracy: 71\n",
      "[Step 300] Past 100 steps: Average Loss 0.7700458257664884 | Accuracy: 78\n",
      "[Step 400] Past 100 steps: Average Loss 0.969602954869667 | Accuracy: 69\n",
      "[Step 500] Past 100 steps: Average Loss 0.7755994959081552 | Accuracy: 79\n",
      "[Step 600] Past 100 steps: Average Loss 0.495628380338773 | Accuracy: 83\n",
      "[Step 700] Past 100 steps: Average Loss 0.6819576660363187 | Accuracy: 80\n",
      "[Step 800] Past 100 steps: Average Loss 0.6226394940552806 | Accuracy: 84\n",
      "[Step 900] Past 100 steps: Average Loss 0.4338760832464212 | Accuracy: 85\n",
      "[Step 1000] Past 100 steps: Average Loss 0.4324004883566357 | Accuracy: 85\n",
      "[Step 1100] Past 100 steps: Average Loss 0.3642780065464558 | Accuracy: 87\n",
      "[Step 1200] Past 100 steps: Average Loss 0.42395910813181104 | Accuracy: 88\n",
      "[Step 1300] Past 100 steps: Average Loss 0.6291504738844144 | Accuracy: 77\n",
      "[Step 1400] Past 100 steps: Average Loss 0.5797129600740918 | Accuracy: 88\n",
      "[Step 1500] Past 100 steps: Average Loss 0.3466441322494497 | Accuracy: 90\n",
      "[Step 1600] Past 100 steps: Average Loss 0.2776102616982991 | Accuracy: 92\n",
      "[Step 1700] Past 100 steps: Average Loss 0.6728525759705388 | Accuracy: 86\n",
      "[Step 1800] Past 100 steps: Average Loss 0.32508803840867556 | Accuracy: 86\n",
      "[Step 1900] Past 100 steps: Average Loss 0.6817099880979132 | Accuracy: 84\n",
      "[Step 2000] Past 100 steps: Average Loss 0.5555809345068065 | Accuracy: 88\n",
      "[Step 2100] Past 100 steps: Average Loss 0.4397184779986862 | Accuracy: 86\n",
      "[Step 2200] Past 100 steps: Average Loss 0.5134880908703292 | Accuracy: 87\n",
      "[Step 2300] Past 100 steps: Average Loss 0.6152415073614044 | Accuracy: 88\n",
      "[Step 2400] Past 100 steps: Average Loss 0.5573779743689175 | Accuracy: 84\n",
      "[Step 2500] Past 100 steps: Average Loss 0.4836124960361453 | Accuracy: 85\n",
      "[Step 2600] Past 100 steps: Average Loss 0.5602046524068487 | Accuracy: 83\n",
      "[Step 2700] Past 100 steps: Average Loss 0.38980989506948466 | Accuracy: 86\n",
      "[Step 2800] Past 100 steps: Average Loss 0.33875755698569626 | Accuracy: 86\n",
      "[Step 2900] Past 100 steps: Average Loss 0.5289821014446406 | Accuracy: 83\n",
      "[Step 3000] Past 100 steps: Average Loss 0.522040265730583 | Accuracy: 86\n",
      "[Step 3100] Past 100 steps: Average Loss 0.7859314986814043 | Accuracy: 76\n",
      "[Step 3200] Past 100 steps: Average Loss 0.4526220809599028 | Accuracy: 86\n",
      "[Step 3300] Past 100 steps: Average Loss 0.7542665874857668 | Accuracy: 84\n",
      "[Step 3400] Past 100 steps: Average Loss 0.5495799037371131 | Accuracy: 87\n",
      "[Step 3500] Past 100 steps: Average Loss 0.2117244755310662 | Accuracy: 93\n",
      "[Step 3600] Past 100 steps: Average Loss 0.5015458329842769 | Accuracy: 87\n",
      "[Step 3700] Past 100 steps: Average Loss 0.39465112357709836 | Accuracy: 88\n",
      "[Step 3800] Past 100 steps: Average Loss 0.2105348039493385 | Accuracy: 90\n",
      "[Step 3900] Past 100 steps: Average Loss 0.2478402403919158 | Accuracy: 91\n",
      "[Step 4000] Past 100 steps: Average Loss 0.4686500323732452 | Accuracy: 88\n",
      "[Step 4100] Past 100 steps: Average Loss 0.3034543911255296 | Accuracy: 87\n",
      "[Step 4200] Past 100 steps: Average Loss 0.38730681694712693 | Accuracy: 90\n",
      "[Step 4300] Past 100 steps: Average Loss 0.5735393954157998 | Accuracy: 83\n",
      "[Step 4400] Past 100 steps: Average Loss 0.36789840388004685 | Accuracy: 91\n",
      "[Step 4500] Past 100 steps: Average Loss 0.17938075629825337 | Accuracy: 95\n",
      "[Step 4600] Past 100 steps: Average Loss 0.39389128212677404 | Accuracy: 89\n",
      "[Step 4700] Past 100 steps: Average Loss 0.25838254560975565 | Accuracy: 93\n",
      "[Step 4800] Past 100 steps: Average Loss 0.3154328744004778 | Accuracy: 89\n",
      "[Step 4900] Past 100 steps: Average Loss 0.49702499369210607 | Accuracy: 91\n",
      "[Step 5000] Past 100 steps: Average Loss 0.2963872480862927 | Accuracy: 90\n",
      "[Step 5100] Past 100 steps: Average Loss 0.4312761580758714 | Accuracy: 90\n",
      "[Step 5200] Past 100 steps: Average Loss 0.26174517503231187 | Accuracy: 93\n",
      "[Step 5300] Past 100 steps: Average Loss 0.40345598085926776 | Accuracy: 89\n",
      "[Step 5400] Past 100 steps: Average Loss 0.3541996331729789 | Accuracy: 92\n",
      "[Step 5500] Past 100 steps: Average Loss 0.36332521577210486 | Accuracy: 89\n",
      "[Step 5600] Past 100 steps: Average Loss 0.24892085113122875 | Accuracy: 91\n",
      "[Step 5700] Past 100 steps: Average Loss 0.4196181198479515 | Accuracy: 90\n",
      "[Step 5800] Past 100 steps: Average Loss 0.33906150281438274 | Accuracy: 90\n",
      "[Step 5900] Past 100 steps: Average Loss 0.1948757671886215 | Accuracy: 92\n",
      "[Step 6000] Past 100 steps: Average Loss 0.24645149605897435 | Accuracy: 93\n",
      "[Step 6100] Past 100 steps: Average Loss 0.5827429317590103 | Accuracy: 86\n",
      "[Step 6200] Past 100 steps: Average Loss 0.7247720333702542 | Accuracy: 85\n",
      "[Step 6300] Past 100 steps: Average Loss 0.38991163862001493 | Accuracy: 86\n",
      "[Step 6400] Past 100 steps: Average Loss 0.21896379995229864 | Accuracy: 94\n",
      "[Step 6500] Past 100 steps: Average Loss 0.2796864368090186 | Accuracy: 89\n",
      "[Step 6600] Past 100 steps: Average Loss 0.42244171042151235 | Accuracy: 92\n",
      "[Step 6700] Past 100 steps: Average Loss 0.4780911139862456 | Accuracy: 90\n",
      "[Step 6800] Past 100 steps: Average Loss 0.24249504518020332 | Accuracy: 92\n",
      "[Step 6900] Past 100 steps: Average Loss 0.3114223856152294 | Accuracy: 92\n",
      "[Step 7000] Past 100 steps: Average Loss 0.4581693831843306 | Accuracy: 87\n",
      "[Step 7100] Past 100 steps: Average Loss 0.5863817186282012 | Accuracy: 84\n",
      "[Step 7200] Past 100 steps: Average Loss 0.41154798122880165 | Accuracy: 89\n",
      "[Step 7300] Past 100 steps: Average Loss 0.21403160832014173 | Accuracy: 93\n",
      "[Step 7400] Past 100 steps: Average Loss 0.3144791353839601 | Accuracy: 89\n",
      "[Step 7500] Past 100 steps: Average Loss 0.244840732212896 | Accuracy: 92\n",
      "[Step 7600] Past 100 steps: Average Loss 0.3007743360814707 | Accuracy: 90\n",
      "[Step 7700] Past 100 steps: Average Loss 0.4896039878521469 | Accuracy: 87\n",
      "[Step 7800] Past 100 steps: Average Loss 0.4510656444862896 | Accuracy: 91\n",
      "[Step 7900] Past 100 steps: Average Loss 0.27635461927904464 | Accuracy: 91\n",
      "[Step 8000] Past 100 steps: Average Loss 0.3399261998719007 | Accuracy: 91\n",
      "[Step 8100] Past 100 steps: Average Loss 0.2398354800195187 | Accuracy: 95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m l, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m     16\u001b[0m accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, y, lr)\u001b[0m\n\u001b[1;32m      4\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m grad[y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mout[y]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(grad, lr)\u001b[0m\n\u001b[1;32m      2\u001b[0m grad \u001b[38;5;241m=\u001b[39m fc_layer\u001b[38;5;241m.\u001b[39mbackward(grad, lr)\n\u001b[1;32m      3\u001b[0m grad \u001b[38;5;241m=\u001b[39m pool_layer\u001b[38;5;241m.\u001b[39mbackward(grad)\n\u001b[0;32m----> 4\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/CNN scratch/CNN_scratch2.py:37\u001b[0m, in \u001b[0;36mConv_Layer.backward\u001b[0;34m(self, d_out, learn_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters):\n\u001b[0;32m---> 37\u001b[0m             d_filters[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_out[i, j, k] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, j:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;66;03m# shape (3, 3)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learn_rate \u001b[38;5;241m*\u001b[39m d_filters \u001b[38;5;66;03m# update the filters with every iteration\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "permutation = np.random.permutation(len(train_X))\n",
    "train_X = train_X[permutation]\n",
    "train_y = train_y[permutation]\n",
    "\n",
    "\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "for i, (X, y) in enumerate(zip(train_X, train_y)):\n",
    "    if(i>0 and i %100 == 99):\n",
    "        print(f'[Step {i+1}] Past 100 steps: Average Loss {loss/100} | Accuracy: {accuracy}')\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "    l, acc = train(X, y, 0.005)\n",
    "    loss += l\n",
    "    accuracy += acc"
   ]
  },
 
   ],
   "source": [
    "test_y.count_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8d37a-7dfc-4626-af77-652b155359b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
